%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Exploring the Semantic Context of News Events  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{llncs}

\newcommand{\superscript}[1]{\ensuremath{^{\textrm{#1}}}}

\usepackage{makeidx}  % allows for indexgeneration
\usepackage[hyphens]{url}
\usepackage{textcomp}
\usepackage{color}
\usepackage{listings}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{pbox}
\usepackage{amsfonts}
\newcommand{\hg}[1]{\colorbox{yellow}{#1}}
\newcommand{\todo}[1]{\colorbox{red}{#1}}

\setcounter{MaxMatrixCols}{20}

% listing styles
\lstset{numbers=left, numberstyle=\tiny,basicstyle=\ttfamily\scriptsize, tabsize=2, keywordstyle=\underbar, stringstyle=\small, backgroundcolor=\color[gray]{0.94}, framexleftmargin=2pt}
\lstdefinestyle{rdfa}{numberblanklines=true, morekeywords={}}

%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Begin Document  %%%
%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\frontmatter          % for the preliminaries
\pagestyle{headings}  % switches on printing of running heads
\mainmatter              % start of the contributions

%\title{Exploring the Semantic Context of News Events}
%\title{Spotting the Hidden Semantics of Newscasts}
\title{Generating Semantic Snapshots of Newscasts using Entity Expansion}
\author{Jos\'e Luis Redondo Garc\'ia\inst{1}, Giuseppe Rizzo\inst{1}, Lilia Perez Romero\inst{2}, Michiel Hildebrand\inst{2}, Rapha\"el Troncy\inst{1}}
\institute{EURECOM, Sophia Antipolis, France, \\
\email{\{redondo, giuseppe.rizzo, raphael.troncy\}@eurecom.fr}
\and
CWI, Amsterdam, The Netherlands, \\
\email{\{L.Perez, M.Hildebrand\}@cwi.nl}
}

\maketitle              % typeset the title of the contribution

%%%%%%%%%%%%%%%%%%
%%%  Abstract  %%%
%%%%%%%%%%%%%%%%%%

\begin{abstract}

% A recent trend in the newscasts domain consists on developing second screen applications that enable the user to further dive and improve his understanding of some particular news items, specially the complex word-scale controversial ones. The making of such applications requires the semantic annotation of the original newscasts as well as semantic enrichment for including background knowledge. Relying on video subtitles for providing time-based annotations of video content is a common method. However, it is insufficient to fully grasp the context of the newscasts being reported, simply because daily news reporting emphasizes on the last facts rather than on the narrative and causes of the reported event. In this paper, we propose a method that enables to search for and analyze related documents in order to generate semantic annotations that are closer to what viewers and experts in the domain expect to find while consuming newscasts. This approach generates annotations in the form of a ranked list of entities. We evaluate this method with respect to a gold standard made by domain experts for a set of BBC newscasts. Results of the experiments show the robustness in semantically annotating newscasts holding an Average Normalized Discounted Cumulative Gain of 66.4\%.

Television newscasts generally report about the latest event-related facts occurring in the world. \textit{Per se} a newscast delivers partial information thus neglecting the whole picture of the event that is often assumed as known. Relying exclusively on the broadcasted news item is therefore insufficient to fully grasp the context of the fact being reported. In this paper, we propose an approach to retrieve and analyze related documents in order to automatically generate semantic annotations that provide viewers and experts of the domain comprehensive information to fully understand the news content. The approach takes as inputs the publication date and the newscast's title for gathering event-related documents on the Web, bringing more representativeness to the available knowledge. Then named entities detected in the retrieved documents are merged with those found in the newscast subtitles for further disclosing hidden relevant entities that were not explicitly mentioned in the original newscast. A ranking algorithm based on entity frequency, popularity peak analysis, and domain experts' rules sorts the annotations to generate the Semantic Snapshot of the considered Newscast (NSS). We benchmark this method against a gold standard generated by domain experts and assessed via a user survey over five BBC newscasts. Results of the experiments show the robustness of the approach holding an Average Normalized Discounted Cumulative Gain of 66.4\%.
%Lilia: not sure whether we can call our videos newscasts or whether they are more online news videos
\keywords{Semantic Video Annotation, Entity Expansion, Newscasts}
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  1. Introduction  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:introduction}
With the emergence of both citizen-based and social media, traditional information television channels have to re-think their production and distribution workflow processes. We live in a globalized world, a vast playing field where events happening are the result of complex interactions between many diverse agents along time. The interpretation of those news is problematic because of two issues. \textit{i)} the \textit{need of background}: viewers probably need to be aware of other facts happened in a different temporal or geographic dimension.
%, like one week ago or in a completely different region. 
\textit{ii)} the \textit{need of completeness}: a single representation of an event is not enough to capture the whole picture, because it is normally incomplete, it can be biased, or partially wrong. 
Recent gadgets and applications, such as second screen devices, have recently irrupt as a good way of assisting the viewer in the challenge of becoming aware of
that bigger picture of the event. In~\cite{Courtois2012}, the authors tracked 260 tablet users, concluding that even though there is a modest uptake and interest in using secondary screens to digitally share opinions, the use of second screen interaction with television content is something the viewer qualitatively appreciate.
%In addition, traditional text-based products are losing market share against other audiovisual solutions, in particular video. 
This opens to a crucial aspect: the second screen interaction needs to be fed with meaningful details concerning a newscast. The most common strategy to get this information is to enrich the original content with additional data collected from external sources: today users have access to multiple news portals, different services for commenting and debating on the news, and social media that instantaneously spread news information. However, this results in large amounts of unreliable and repeated information, leaving to the user the burden of processing the large amounts of potentially related data to build an understanding of the event.
%in the middle of an ocean of rumors or hoaxes.

%\todo{GR: this is a jump -> from means (video, ...) to entities so why?}
Machine driven approaches have tried to alleviate the human difficulties when navigating this huge amount of data, but they struggle both in finding a good set of candidate documents, and filtering them.
% in order to offer relevant data back to the users. 
One strategy reported in the literature for having such a mechanism is to perform named entity extraction over the newscast transcript [\todo{add REF}]. However, the set of named entities obtained from such an operation is insufficient and incomplete for expressing the context of a news event [\todo{add REF}]. Sometimes entities spotted over a particular document are not disambiguated because the textual clues surrounding the entity are not precise enough for the name entity extractor. While in some others, entities are simply not mentioned in the transcripts while being relevant for understanding the story. This is also an inherent problem in information retrieval tasks: a single description about the same resource does not necessarily summarize the whole picture. In this paper we automatically retrieve and analyze additional documents from the Web where the same event is also described. By increasing the size of the document set to analyze, we increase the completeness of the context and the representativeness of the list of entities, reinforcing relevant entities and finding new ones that are potentially interesting inside the context of that news item. This approach is able to produce a ranked list of entities called Newscast Semantic Snapshot (NSS), which includes the initial set of detected entities in subtitles with other event-related entities captured from the seed documents. 
%The NSS is later used to feed second screen applications assisting the consumer who watches the news, summarizing and generating quality snippets, or launch further  enrichment processes who bring related content the users could be also interested in watch. 

The paper is organized as follows: Section~\ref{sec:RelatedWork} presents the related works, Section~\ref{sec:Approach} illustrates the approach depicted in this paper for generating NSS. Section~\ref{sec:Ranking} describes in depth the different ranking algorithms used for ordering the list of candidate entities generated in previous steps. In Section~\ref{sec:GoldStandard} we report about the creation of the gold standard used for evaluating the NSS. The experimental settings are described in Section~\ref{sec:Evaluation}. We conclude with Section~\ref{sec:Conclusion} summarizing the main findings and outlining the future plans.

% State of the Art on Semantic annotation
%* State of the Art on Semantic annotation.
%Concerning the multimedia annotation the literature covers a wide range of different analysis techniques~\cite{ballan2011event}. One of the main approaches consists on running Named Entity Recognition (NER) over the textual information attached to particular video fragment. Those techniques are an essential component within the Information Extraction field that focus on: identifying atomic information units in texts, named entities; classifying entities into predefined categories (also called context types) and linking to real world objects using web identifiers (Named Entity Disambiguation). A growing number of APIs provide such a service, like AlchemyAPI\footnote{\fontsize{8pt}{1em}\selectfont \url{http://www.alchemyapi.com/}} or DBpedia Spotlight\footnote{\fontsize{8pt}{1em}\selectfont \url{http://spotlight.dbpedia.org/}}. 


%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  2. Related Work  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}
\label{sec:RelatedWork}
%Studies on second screen applications and tools for supporting journalists. (WHY)
%\todo{GR:the first 4 lines are simply blabla and can be removed without loss}
%Television content in general and newscast in particular need the assistance of innovative applications which help the viewers and experts to get the most of them. One implementation design that has been deeply studied in the last years, mainly due to the increasing availability of tablets and tactile device is the "second screen application" paradigm. This kind of experiences open new possibilities for users watching television by complementing what is being displayed on the screen and offering a more active interaction with the content. 

%The need of a semantic snapshot.
The need of a NSS for feeding certain applications is already something we have experimented in previous research works and prototypes \cite{Redondo2014}, which have revealed the great benefit for users of browsing the ``surrounding context'' of the newscasts. The same concept\footnote{\url{https://vimeo.com/119107849}} has been presented to the forum Iberoamerican Biennial of Design (BID).\footnote{\url{http://www.bid-dimad.org}}
% with great reviews from the experts and scoring between the final 5 from a total of 200 candidates. \\
Research efforts have underlined the importance in professional journalism of algorithms, data, and social science methods for reporting and storytelling, under the term of computational exploration in journalism ~\cite{gynnild2014}.  %CEJ demands precise event descriptions for supporting the journalistic co-creation of quantitative news projects that transcend geographical, disciplinary, and linguistic boundaries.
%%Entities in making sense (Semantic snapshot)  (WHAT)
%All those news applications need to be fed with the appropriate data to effectively assist viewers and professionals, and this information is normally broader than the one explicitly offered by the content itself. The hypothesis presented in this research paper states that this required knowledge can be represented in the form of a list of named entities called Newscast Semantic Snapshot (NSS). 
%This idea of using semantic annotations and named entities in particular is not new. 
Projects like NEWs have studied how to disambiguate named entity in the news domain by continuously learning while processing news streams~\cite{Fernandez2012}. In the domain of Social Networks, named entities are also used for identifying and modeling events and detecting breaking news. In~\cite{Steiner2013}, the authors emphasize the importance of spotting news entities in short user generated post in order to obtain a better understanding about what they are talking about. Entities have been used for the video classification when the textual information attached to a video contains temporal references (e.g. subtitles)~\cite{yunjia2013}. 

%How to create this semantic snapshot (Expansion idea) (HOW)
In order to build a comprehensive NSS the knowledge expressed in the newscast has to be extended with further details. Our approach performs an entity expansion process, which allows to collect on the fly event-related documents from the Web. 
In the literature there are already some approaches relying in similar expansion techniques, even if the final objective is not spotting other event-related entities: instead, they transform the feature space from a from small number of named entities with the same type or category to a more complete named entity set. One of them is Google Sets.\footnote{\url{http://googlesystem.blogspot.fr/2012/11/google-sets-still-available.html} not longer available since 2014.} Set expansion using the Web is also closely related to the problem of unsupervised relation learning~\cite{Cafarella2005}, and set-expansion-like techniques have been used to derive features for concept-learning~\cite{Cohen2000}, to construct ``pseudo-users'' for collaborative filtering~\cite{Cohen99}, and to compute similarity between attribute values in autonomous databases~\cite{Wolf2007}. In~\cite{Wang2007}, authors proposed the Set Expander for Any Language (SEAL) approach. SEAL works by automatically finding semi-structured web pages that contain lists of items and the aggregating these list in a way that the most promising items are ranked higher. SEAL is a language-independent system and it has shown good performance against previously published results like the already mentioned Google Sets. By using particular seeds and the top one hundred documents returned by Google, SEAL achieves 93\% in average precision in dataset from various languages. The same authors published an improved version of the algorithm~\cite{Wang2008}, increasing the performance by handling unlimited number of supervised seeds. In each iteration, it expands a couple of randomly selected seeds while accumulating statistics from one iteration to another. Our approach does not rely on such kind of iterative mechanism, and focus more in maximize the quality of the search query for obtaining the most appropriate set of related documents to be analyzed. Another approach in extending set of entities is \cite{Mai-Vu2010}, which combines the power of semantic relations between language terms like synonymy and hyponymy and grammar rules in order to find additional entities in the Web sharing the same category that the ones provides as input. Relying in Google they analyze documents for parsing semistructured text elements like tables and rank the final candidates using different ranking algorithms like PageRank. Numerous approaches have dealt with a set expansion method using free text rather than semi-structured Web documents; for instance authors in \cite{Talukdar2006} presented a method for automatically selecting trigger words to mark the beginning of a pattern, which is then used for bootstrapping from free text. But still this approach looks for category related entities while in our case the driving force is more the event being shown in the news item. A related work has been carried out grounding the power of enriching the set of initial entities by using an entity expansion algorithm~\cite{RedondoGarcia2014}. It includes a naive document collection strategy, it proposes an entity ranking algorithm based on the appearance of the entities in the collected documents, and it exploits the DBpedia knowledge base as a way to ensure the coherence of the final list of entities. The work presented in this paper improves the referred work in several directions: document retrieval mechanism, semantic annotation, creation of the NSS. \cite{RedondoGarcia2014} is used as one of the baselines in the experimental settings.%\hg{they are missing some examples of SSN and the related work made by Lilia}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  3. News Concept Expansion Approach  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{News Concept Expansion Approach}
\label{sec:Approach}
%In this section we present our algorithm to automatically generate News Semantic Snapshots out of a particular newscast. 
The approach we use to generate Newscast Semantic Snapshot is composed of the following stages: query formulation, document retrieval, semantic annotation, annotation filtering, and annotation ranking. Fig.~\ref{fig:namedEntityExpansion} depicts the process.

\todo{GR:names in Fig aren't consistent: either you use all lowercase of all uppercase}
\todo{GR:normalize the names in Fig with the ones reported in the paper}
\begin{figure}[h!]
\centering
\includegraphics[width=1\textwidth]{figure/ExpansionDiagram}
\caption{Schema of Named Entity Expansion Algorithm.}
\label{fig:namedEntityExpansion}%\end{figure}
\end{figure}

{\bf Query Formulation} %OLD APPROACH
%The \emph{Five W's} is a popular concept of information gathering in journalistic reporting. It captures the main aspects of a story: who, when, what, where, and why~\cite{LiJia2007}. We try to represent the news item in terms of four of those five W's (who is involved in the event, where the event is taking place, what the event is about, and when it has happened) in order to generate a query that retrieves documents associated to the same event.
%First, the original entities are mapped to the NERD Core ontology, which considers 10 main classes: Thing, Amount, Animal, Event, Function, Organization, Location, Person, Product and Time. From those ten different categories, we generalize to three classes: the Who from \url{nerd:Person} and \url{nerd:Organization}, the Where from \url{nerd:Location}, and the What from the rest of NERD types after discarding \url{nerd:Time} and \url{nerd:Amount}.
Newscast broadcasters offer a certain amount of metadata about the items they publish, which is normally available together with the audiovisual content itself. In this work, we build the query $q= \left [ \text{h}, \text{t} \right ]$, where \textit{h} is the video heading, and \textit{t} is the publication date. The query is then used to as input of the retrieval stage.
%newscasts have with the following metadata: title, publication date, and the video transcript (non necessary timed). 
%We model a newscast as:
%\begin{equation}
%\text{q} =\left [ \text{t}, \text{d} \right ]
%\end{equation}
%Those different fields are taken from the video publisher and directly used as input for the rest of the expansion workflow without any pre-processing.

{\bf Document Retrieval}
The retrieval stage has the intent to collect event-related documents from the open Web. To some extents this process emulates what a viewer, who misses some details about the news he is watching, does: going to the Web, make a search, and get the most of the top ranked documents. Our programmatic approach emulates this human driven task by analyzing a much bigger set of related documents in a drastically smaller amount of time. The stage consists of retrieving documents that report on the same event discussed in the original video as result of the query \textit{q}. This phase in the expansion process has an key role in the upcoming semantic annotation stage, since it selects a set of the documents \textit{D} over which the semantic annotation process is performed. The quality and adequacy of the collected documents sets a theoretical limit on how good the process is done. 
%The more appropriate the collection of related document is, the better the final results will be.
%The different behavior of search engines make some alternatives more suitable than others for certain kinds of events. The way the resulting documents change in the search engines for a particular kind of event is a research question that will not be studied in this paper. 

{\bf Semantic Annotation} In this stage we perform a named entity recognition analysis with the objective of reducing the cardinality of the textual content from the set $D$ of documents $\{d_1, ..., d_n, d_{n+1}\}$ where $d_{i=1,..,n}$ defines the $i_{th}$ retrieved document, while $d_{n+1}$ refers to the original newscast transcript. Since most of the documents retrieved are Web pages, HTML tags and other annotations are removed, keeping only the main textual information. The feature space is then reduced and each document $d_i$ is represented by a bag of entities $E_{d_i}={e_{1_{d_i}}, ..., e_{n_{d_i}}}$, where each entity is defined as a triplet $(surface\_form, type, link)$. We perform a union of the obtained bags of named entities resulting in the bag of entities $E$ of the initial query $q$. 

{\bf Annotation Filtering and Clustering} The Document Retrieval stage expands the content niche of the newscast. At this stage we apply coarse-grained filtering of the annotations $E$ obtained from the previous stage, applying a $f\left ( E_{d_i}\right )\rightarrow  E'_{d_i}$ where $\left |E'_{d_i}  \right | < \left |E_{d_i}  \right |$. The filtering stategy grounds on the findings we obtained in the creation of the gold standard. In fact, when watching a newscast viewers better capture Person-type entities, as well as Organization-type and Location-type entities. The rest of less-specific and wider enclosed entities are more vague to be displayed on a television user interface and potentially less relevant for complementing the seed content. 
Named entities are then clustered applying a centroid-based clustering operation. As cluster centroid we consider the entity with the most frequent disambiguation $link$ that also have the most repeated $surface\_form$. As distance metric for comparing the instances, we applied strict string similarity over the $link$, and in case of mismatch, the Jaro-Winkler string distance~\cite{winkler2006overview} over the $surface\_form$. The output of this phase is a list of clusters containing different instances of the same entity.

{\bf Semantic Annotation Ranking}
%The different related documents collected and annotated in previous steps of the expansion algorithm contain the pieces of the puzzle that will allow to re-construct the big picture of the event. Unfortunately, those pieces come wrapped together with other less important parts which are not closely related with the event in particular and need to be put aside. By ranking the concepts, we make easier to (1) identify important entities from those which are, (2) group them in the top positions in order, (3) easy get rid of those which are not relevant to the considered event. The final NSS will be a subset of the top scored entities after the ranking algorithm.
The bag of named entities $E'_{d_i}$ is further processed to promote the named entities which are highly related to the underlined event. To accomplish such an objective, a we propose a ranking strategy based on entity appearance in documents, popularity peak analysis, and domain experts' rules sorts the annotations to generate the Semantic Snapshot of the considered Newscast (NSS).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  4. Ranking Strategy %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ranking Strategy}
\label{sec:Ranking}
The unordered list entities is ranked to promote the entities that are potentially interesting for the viewer. The strategy we present in this work grounds on the assumption that the entities which appear often in the retrieved documents are important. We propose two different scoring functions for weighting the frequency of the entities. We then discuss two orthogonal functions which exploit the entity popularity in the time window considered for the event, and the domain experts' rules.
%and get rid of the long queue of less relevant concepts and noise generated by during the collection process. At the same time, this is also the most complicated task to perform: while computers are good in processing much large amounts of information and it would be possible to analyze hundreds of related documents, to decide which of the generated entities would be selected as relevant by a human is much more complicated. 

\subsection{Frequency-based Function}
The first intuitive way of ranking entities is according to their absolute frequency within the set of documents $D$. Let define $f_a(e_i, D)=[0,..,1]$, we define the score $S_F = \frac{ f_a(e_i, D) }{ |E| } $, where $|E|$ stands for the cardinality of all entities spotted across all documents. 
%Following this criteria, the more mentioned an entity is, the higher is will be scored. The opposite occurs to those barely mentioned all along the corpora, which will drop down in the final list of candidates. Therefore we have implemented a ranking strategy that orders entities according to their absolute frequency all over the collected documents: $S_{F}\left ( e \right ) =  f_{a}(e, D)$. 
%A second version of this simple strategy considers also confidence scores coming form the semantic annotation phase, in order to consider the certainty of the spotting process when detecting the entity $R_{score}\left ( e \right ) =  \sum_{1}^{i}c_{e_{i}}$ . 
In Fig.~\ref{fig:rankingStrategies} (a) we can observe how entities with lower absolute frequency are placed at the beginning of the distribution and discarded in the final ranking, where those with high $S_F$ are situated on the right side of the plot and they are elected to be part of the NSS.

\subsection{Gaussian-based Function}
Following a similar principle of the $S_F$, if an entity appears in a big number of documents, then it means the entity is relevant to be shown. However from the perspective of a television viewer, this is not always true: while it is true that entities appearing in just a few documents are probably irrelevant and not representative enough to be considered in the final results, entities spread all over the whole set of related documents are not necessary something the viewers would need to know about. In fact, they often represent entities that have been so present in media before that they have become too much obvious to the viewer. As consequence, we measure the Bernoulli appearance value of the entities across all documents as $f_{doc}(e_i)$, resulting in a Gaussian distribution where entities are condensed around the mean $\mu = \frac{|D|}{2}$ where $|D|$ is the cardinality of the retrieved documents (Fig.~\ref{fig:rankingStrategies} (b)). We have then approximated this behavior by using the function: $S_G = 1-\left | \frac{ f_{doc}(e_i) }{|D|} -1 \right |$.

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{figure/RankingStrategies}
\caption{(a) depicts the Decay function of the entity occurrences in the corpus, and the $S_F$ which underlines the importance of an entity being used several times in the corpus. (b) represents the Gaussian-based function $S_G$, with the entities highly impontant over the mean.}
\label{fig:rankingStrategies}%\end{figure}
\end{figure}

% Random Walks
% PageRank
% Use prior knowledge graph


\subsection{Orthogonal Functions}
\subsubsection{Popularity Function}

\todo{GR: separate the formalization part from the experimental settings, such as using GT, etc etc}
%Even the three ranking algorithms follow different considerations and principles, they end up exploiting the most precise an representative frequency values that the entity expansion process can provide compared to any other analysis over more reduced and probably partial newscast descriptions. 
We propose a weighting function based on a mechanism that detects variations in entity popularity values over a time window (commonly named as popularity peaks) around the date of the studied event. The functions proposed above exploit the frequency of the entities in documents as factor to measure its importance. However the frequency based approaches fail to explain the phenomenon of certain found entities which are barely mentioned in related document but suddenly become interesting for viewers. These changes are sometimes unpredictable so the only solution is to rely on external sources that can provide indications about the entity popularity, like Google Trends\footnote{\url{https://www.google.es/trends}} or Twitter\footnote{\url{https://twitter.com/}}. 

\begin{figure}[h!]
\centering
\includegraphics[width=0.7\textwidth]{figure/PopularityMeasure}
\caption{\todo{add proper caption}}
\label{fig:PopularityMeasure}%\end{figure}
\end{figure}

The procedure for getting $P_{peak}(e_i)$ is depicted in Fig.~\ref{fig:PopularityMeasure}. Using the label of an entity $e_i$, we obtain a list of pairs $[t, P]$ where $P\in[0,100]$ is the popularity score of an entity at the instant of time $t$. Afterward we create three consecutive and equally long temporal windows around $t$, the first one $w_{t}$ containing the date itself, another one just immediately behind $w_{t-1}$ and a last one after the previous two $w_{t+1}$. 
In a next step we approximate the area inside the regions by calculating the average of the points contained in them, obtaining $\overline{w-1}$, $\overline{w}$ and $\overline{w+1}$. The slopes of the lines between $\overline{w-1}$ and $\overline{w}$, and $\overline{w}$ and $\overline{w+1}$ give the values $m_{up}$ and $m_{down}$ respectively, which are normalized and combined into a single score for measuring how significant the variation in volume of searches was for that studied entity label. When aggregating those two gradient, we scored $m_{up}$ higher in order to emphasize the irruption of a change, more than the posterior distribution of the search term.
%\todo{GR:better phrase this part. This is a pure approximation btw}

By empirically studying the distribution of the popularity scores of the entities belonging to a newscast, we have observed that it follows a Gaussian curve. This fact will help us to better filter out popularity scores that do not trigger valid conclusions and therefore improving the merging of the ranking produced by the previous functions with the outcome from the popularity peaks detection algorithm. 

\subsubsection{Expert Rules Function}
%\todo{GR: dunno if it's propoer having this here, before the entire explaination. Most of the section should be moved later in the settings - supervised thing}
The knowledge of experts in the domain, like journalist or newscast editors can be materialized in the form of rules that correct the scoring output produced by former ranking strategies. The antecedent of those rules is composed by entity features like type, number of documents where they appear, or the Web source from where they have been extracted from, while the precedent will involve the recalculation of the scoring function according to the following equation: $S_{expert}\left ( e \right )^{*} =  S_{F-1}\left ( e \right ) *Op _{expert}$, being $Op _{expert}$ a factor which models the domain expert's opinion about the entities that match in the antecedent.


%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  5. Gold Standard %%%
%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gold Standard for Evaluating Newscast Semantic Snapshot}
\label{sec:GoldStandard}
We are interested in evaluating ranking strategies for generating semantic snapshots of newscasts, where each snapshot is charactherized by a set of named entities. We narrowed down the selection of named entity types to Person, Organisation, and Location, since they can be directly translated in \textit{who}, \textit{what}, \textit{when}, a subset of the 5Ws well-known questions in the journalisms. To the best of our knowledge there is no evaluation dataset suited to this context. The title of the newscasts and the breakdown figures per entity type are shown in Table~\ref{table:entitydistribution}. The dataset is freely available\footnote{\url{\todo{ADD LINK to the DATASET}}}.

%\vspace{-1.5em}
\begin{table}
\begin{tabular}{| p{6cm} | l| l| l| l|}
  \hline
  \textbf{Newscast Title} & \textbf{Person} & \textbf{Organisation} &\textbf{Location} & \textbf{Total} \\
    \hline
  Fugitive Edward Snowden applies for asylum in Russia & 11 & 7 & 10 & 28 \\
    \hline
 Egypt's Morsi Vows to Stay in Power & 4 & 5 & 4 & 17 \\
    \hline
 Fukushima leak causes Japan concern & 7 & 5 & 5 & 13\\
    \hline
 Rallies in US after Zimmerman Verdict & 9 & 2 & 8 & 19 \\
    \hline
 Royal Baby Prince Named George & 15 & 1 & 6 & 22 \\
    \hline
    \textbf{Total}  & 46 & 20 & 33 & 99\\
  \hline
\end{tabular}
\caption[Table caption text]{Breakdown entity figures per type and per newscast.}
\label{table:entitydistribution}
\end{table}

%\vspace{-2em}
\subsection{Newscast Selection}
We randomly selected 5 newscasts from the BBC One Minute World News website\footnote{\url{http://www.bbc.com/news/video_and_audio/}}. Each newscast lasted from 1 to 3 minutes. The selection covered a wide range of subjects specifically: politics, armed conflicts, environmental events, legal disputes, and social news. 
%The intention behind this topic choice was to fit international audiences, since we planned to perform a user study with international participants. 
Subtitles of the videos were not available; therefore, a member of the team manually transcribed the speech in the newscasts.
%After obtaining the transcriptions, the following steps were performed in order to obtain a set of unbiased candidate entities. 

\subsection{Newscast Semantic Annotation}
The annotation process involved two annotators and one expert of the domain. The output of this stage is a list of entity candidates. The two annotators were asked to detect for each newscast entities from: 
\begin{description}
\item[subtitle]: the newcast subtitle;
\item[image]: every time a recognisable person, organisation or location was portrayed in the newscast, the entity was added to the list;
\item[image captions]: the named entities appearing in such tags, such as nametag overlays, were added to the candidate set;
\item[external documents]: the two annotators were allowed to use Google Custom Search to look for articles related to the video. The query followed the pattern: title of the newscast, date. The sources were considered: The Guardian, New York Times, and Al Jazeera online (English). The results were filtered of one week time, where the median is represented by the day when event took place.
\end{description}

A journalist, with more than 6 years of experience as a writer and editor for important American newspapers and websites, acted as the expert of the domain. He was asked to watch the newscasts and to identify for each the entities either mentioned or not that better serve the objective of showing interesting additional information a final reader. The expert did not have access to the candidate set built by the two annotators, and he was completely free to suggest any named entity he wanted. 

\subsection{Quality control and Ranking}
A quality control refined the set of entities coming from the previous stage, eliminating all named entity duplicates and standardising names. 
%For example, when we had ``Barack Obama'' as an entity and ``Obama'' as another entity we eliminated the shorter one and left the complete name. A total of 99 entities were obtained from all videos. 
We then conducted a crowdsourcing survey with the objective to gather information about the degree of interestingness of the entities for each newscast. Based on \cite{vonBrzeski:2007:LCU:1321440.1321537} we define interestingness whether an entity is interesting, useful or compelling enough to tear the user away from the main thread of the document. Fifty international subjects participated in this online study. They responded an online call distributed via email and social networks. Their age range was between 25 and 54 years with an average age of 30.3 (standard deviation 7.3 years). 18 participants were female and 32 were male. Most of the participants were highly educated and 48 of them had either a university bachelor degree or a postgraduate degree. The main requisite for participation was that they were interested in the news and followed the news regularly, preferably through means that include newscasts.
During the interview participants were asked to choose at least 3 out of 5 videos according to their preferences. Then they were shown each one of the newscasts. Then they were asked to rate whether they would be interested in receiving more information about the named entities in the context of the news video and on a second screen or similar application. All the named entities from the candidate set related to the last seen video were shown in a list with ratio buttons arranged in a similar way to a three-point Likert-scale. The possible answers were ``Yes'' ``Maybe'' and ``No''. 

\todo{JL:Transforming to scores from 2 to 0}
\todo{JL:Period of time where the events were happening?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  6. Experimental Settings and Evaluation %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Settings and Evaluation}
\label{sec:Evaluation}

In this section we measure the effectiveness of our approach for building the NSS of a newscast against the ground truth shown before. In first place, we justify the mesures considered to carry out the study. Afterwards, we have specified the experimental settings used in the different runs. Finally las subsection shows a table with the top runs, a comparison with two considered baselines, and a discussion which parameters in the algorithm configuration have contributed the most in increasing the performance.

\todo{GR:main outcome in a few words, and then list the different points}

\subsection{Measures}

Inspired in similar studies in Web search engines, we have based our evaluation procedure in measures which try to find as many relevant documents as possible, while  keeping the premise that the top ranked documents are the most important. In order to summarize the effectiveness of a the different algorithm across the entire collection of queries considered in the Ground Truth, we have considered different averaging measurements that are listed below:

 \begin{itemize}
\item Mean precision/recall at rank N. It is the probably the most used measure in this information retrieval task. It is easy to understand and emphasizes the top ranked documents. However it does not distinguish between differences in the rankings at positions 1 to p, which may be considered important for some tasks. For example, the two rankings in Figure~\ref{fig:precisionRecall} will be the same when measured using precision at 10.
\item Mean average precision at N. Also called $MAP$, it takes in consideration the order of the relevant items in the top N positions and is an appropriate measure for
evaluating the task of finding as many relevant documents as possible, while still reflecting the intuition that the top ranked documents are the most important.
\item Average Normalized discounted cumulative gain at $N$. $MNDCG$ has become a popular measure for evaluating Web search and related applications\cite{croft2010}. It is based in the assumption that there are different level of relevance for the documents obtained in results. According to this, the lower the ranked position of a relevant document the less useful it is for the user, since it is less likely to be examined. 
 \end{itemize}

As the relevant documents in our Ground Truth are scored in relevance for the user, we have mainly focused on the last mesure since it can provide a more exhaustive judgement about the adequacy of the generated SNN. Concerning the evaluation point N, we have performed an empirical study over the whole set of queries and main ranking functions observing that from  $N=0$ NDCG decreasingly improves until it reaches a stable behavior from $N=10$ on. Finally, we will not perform measures in terms of efficiency. Even this kind of studies are easier to quantify, this falls outside the scope of this paper.


\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{figure/precisionRecall}
\caption{Inability of $P/R$ for considering the order of the relevant documents: rankings 1 and 2 share the same Precision and Recall at 10 .}
\label{fig:precisionRecall}%\end{figure}
\end{figure}

\subsection{Experimental Settings}
\label{sec:experimentalSettings}
\subsubsection{Document retrieval}
We have relied on the Google Custom Search Engine API service\footnote{\fontsize{8pt}{1em}\selectfont  \url{https://www.google.com/cse/all}} by launching a query with the parameters specified by $q= \left [ \text{h}, \text{t} \right ]$. Apart of the query itself, the CSE engine considers other parameters that need to be tuned up. First, due to quota restrictions the maximum number of retrieved document is set to 50. But in addition, we have also considered 3 different dimensions that could potentially influence the effectiveness in retrieving related documents:
\begin{enumerate}
 \item Web sites to be crawled. Google allows to specify a list of web domains and subdomains where documents can be retrieved from. This reduces the scope of the search task and, depending on the characteristics of the considered sources, influence the nature of the retrieved items: from big online newspapers to user generated content. At the same time, Google allows to prioritize searching over those whitelists while still considering the  whole indexed Web. Based on this, in our study we considered five possible values for this parameter:
a) AllGoogle: search over the whole set of Web pages indexed by Google.
 \begin{itemize}
  \item Newspaper whitelist. A set of 10 internationals English speaking newspapers chosen from \footnote{\fontsize{8pt}{1em}\selectfont  \url{http://en.wikipedia.org/wiki/List_of_newspapers_in_the_world_by_circulation}}
  \item Lilia's whitelist. A set of 3 international newspapers used in [Lilia] while building the ground truth used in Section~\ref{sec:evaluation}.
  \item Newspaper whiteList + Google. Prioritize content in Newspaper whitelist but still consider other sites.
  \item Lilia's whitelist + AllGoogle: Prioritize content in Lilia's whitelist but still consider other sites.
 \end{itemize}
 \item Temporal dimension. This variable allows to filter those documents which are not temporarily close from the day where the newscast was published. Assuming that the news item is fresh enough, this date of publication will also be fairly close to the day the event  took place. Taking $\text{PublishingDate}$ as a reference and increasing the window in a certain amount of days $d$,  we end up having $Time_{Window}=\left [ \text{t}-d, \text{t}+d \right ]$ The reason why we expand the original event period is because documents concerning a news event are not always published during the time of the action is taking place but some hours or days after or before. The final $Time_{Window}$ could vary according to many factors such as the nature of the event itself (whether it is a brief appearance in a media, or part of a longer story with more repercussion) or the kind of documents the search engine is indexing (from very deep and elaborated documents that need time to be published, to short post quickly generated by users). In this study we have considered two possible values for it: 2 weeks and one week temporal windows.
 \item In addition, Google Custom Search Engine makes possible to filter result according to the Schema.org types: for our experiment we foresee the following configuration values: [NoFilterer, Person\&Organization Filtered]
\end{enumerate}

That makes in total $5 * 2 * 2 = 20$ different parameter configurations that will be considered 
that we will study in the Section~\ref{sec:evaluation} in order to discover which configuration optimizes the expansion algorithm.

\subsubsection{Semantic Annotation}
\label{sec:settingsFilteringClustering}
\todo{JL:Particularities of NERD-ML reported here??}

\subsubsection{Annotation Filtering and Clustering}
\label{sec:settingsFilteringClustering}

After some first trials it became evident that there were many non-pure Named Entities detected in the semantic annotation phase which are not well considered by viewers and expert during in ground truth. Those were dramatically dropping the scores for most of the considered ranking strategies. We have foreseen three different filtering approaches for getting rid of undesired concept annotations that can potentially drop down the scores for most of the considered semantic ranking function:

\begin{enumerate}
\item Filter F1. Filter annotations according to their NERD type. In our case, we keep only Person, Location, and Organization.
\item Filter F2. Consist on getting ride of entities with confidence score under first quarter of the distribution.
\item Filter F3. Intuitively, people seems to be more attracted by proper names than general terms. Those names are normally capitalized. This filter keeps only concepts matching this rule.
\end{enumerate}

By concatenating those filters, we obtain the following combinations: F1,  F2,  F3,  F1\_F2,  F1\_F3, F2\_F3, F1\_F2\_F3 ). In order to reduce the number of runs to be launched, we did a first preselection of filters by setting the rest of steps of the approach to default values and averaging the scores obtained over the different queries. We ended up discovering that 3 of the filters (F1 and F3, and the combination F1\_F3) were producing the bigger improvement in the final MNDCG, 

\subsubsection{Semantic Annotation Ranking}
\label{sec:settingsAnnotationRanking}

For the current experiment we will run both frequency and Gaussian based functions, together with the orthogonal strategies based on popularity and expert rules. This makes a total of $2*2$ possible ranking configurations that will be considered and reported in next Subsection~\ref{sec:results}.

Regarding the particular details of the orthogonal functions, we have proceeded as follow:

\begin{itemize}
\item Popularity: we have relied on Google Trends\footnote{\url{https://twitter.com/}}, which estimates how many times a search-term has been used in a given time-window.
Given that Google Trends is giving results with a monthly temporal granularity, we have fixed the duration of such $w$ to 2 months in order to increase the representativity of the samples without compromising too much the validity of the selected values according with the time the event took place. With the aim of being selective enough and keep only those findings backed by strong evidence, we have filtered the entities with peak popularity value higher than $\mu+2*\sigma$ which approximately corresponds to a 2.5\% of the distribution. Those entities will have their former scores combined with the popularity values via the the following equation: $S_{P}\left ( e \right )^{*} =  R_{score}\left ( e \right ) +Pop_{peak}(e)^{2} $.
\item Expert Rules: 
\begin{enumerate}
%$S_{expert}\left ( e \right )^{*} =  S_{F-1}\left ( e \right ) *Op _{expert}$, being $Op _{expert}$ 
\item Entity type based rules: we have considered three rules to be applied over the three entity types considered in the ground truth. The different indexes per type have been been deduced by relying on the average score per entity type in the gold standard $\overline{Sgt}_{entityType}$. 
%with the average score all over the entities $\overline{S}_{GT}$, so $Op _{expert}=1+(\overline{Sgt}_{GT_{entityType}} - \overline{S}_{GT})$ . 
Organizations have gotten the better weight ($Op _{expert}=0.95$), followed by Persons ($Op _{expert}=0.74$), and by Locations ($Op _{expert}=0.48$) that are badly considered and therefore lower ranked in general.
%-0.292803855
%- Organizations have the higher average score compared to the average, so they are eminently interesting for users. -0,05
%- Persons have a fair average compare to the mean, so they should be somehow promoted as well. -0,24
%- $GT_{Location}$ is quite low compared to the average so scores of entities from this type should be penalized. -0,52.
\item Entity's documents based rules: each entity has to appear at least in two different sources in order to become a candidate. All entities whose document frequency $f_{doc}(e_i)$ is lower than 2 are automatically discarded ($Op _{expert} = 0$).
\end{enumerate}
\end{itemize}


\subsection{Results}
\label{sec:results}

That makes $5 * 2 * 2 = 20$ different runs 

20 * 4 * 3
ranked according to 
in addition we showed 2 baselines
Baselines.

\subsubsection{TFIDF-based Function}
We measure the importance an entity in a document over a corpus of documents $D$ (reducing the absolute frequency of the entity) using the TF-IDF. This measure penalizes those entities appearing more frequently in general. The function is then as follows:

\begin{equation}
\begin{matrix}
tf(e_i,d_j) = 0.5 + \frac{0.5\times f_{a}(e_i,D)}{max\left \{ f_{a}(e_i',D) : e_i' \in d_j\right \}},   idf(e_i,d_j) = log\frac{\left | D \right |}{\left \{ d_j\in D  :  e_i\in d_j \right \}}
\end{matrix}
\end{equation}

However in our case the ranking algorithm has to be able to expose the importance of an entity inside the whole corpus $D$, while by definition TF-IDF refers only to the importance of an entity inside a particular document. 
For this reason we have extended this measure to capture the whole context of the newscast by aggregating the different $tf(e_i,d_j) \times idf(e_i,d_j)$ into a single function $tfidf^{*}(e_i,D)$ via the function $S_{TFIDF}= \frac{ \sum_{j=1}^{n} \sum_{i=1}^{n} tf(e_i,d_j) \times idf(e_i,d_j)} {|D|}$.


\begin{table}[h]
\begin{tabular}{llllllllllll}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Run}} & \multicolumn{3}{c|}{Collection} & \multicolumn{1}{c|}{\multirow{2}{*}{Filtering}} & \multicolumn{3}{c|}{Functions} & \multicolumn{4}{c|}{Result} \\ \cline{2-4} \cline{6-12} 
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Sources} & \multicolumn{1}{c|}{$T_{Window}$} & \multicolumn{1}{c|}{Schema.org} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{Freq} & \multicolumn{1}{c|}{Pop} & \multicolumn{1}{c|}{Exp} & \multicolumn{1}{c|}{$MNDCG_{10}$} & \multicolumn{1}{c|}{$MAP_{10}$} & \multicolumn{1}{l|}{$MP_{10}$} & \multicolumn{1}{l|}{$MR_{10}$} \\ \hline
Ex0 & Google & 2W&   & F1+F3 & Freq &   & \checkmark & 0.666 & 0.71 & 0.7 & 0.37 \\
Ex1 & Google & 2W&   & F3 & Freq &   & \checkmark & 0.661 & 0.72 & 0.68 & 0.36 \\
Ex2 & Google & 2W&   & F3 & Freq & \checkmark & \checkmark & 0.658 & 0.64 & 0.6 & 0.32 \\
Ex3 & Google & 2W&   & F3 & Freq &   &   & 0.641 & 0.72 & 0.74 & 0.39 \\
Ex4 & L1+Google & 2W&   & F3 & Freq &   & \checkmark & 0.636 & 0.71 & 0.72 & 0.37 \\
Ex5 & L2+Google & 2W&   & F3 & Freq &   & \checkmark & 0.636 & 0.72 & 0.7 & 0.36 \\
Ex6 & Google & 2W&   & F1+F3 & Freq &   &   & 0.626 & 0.73 & 0.7 & 0.38 \\
Ex7 & L2+Google & 2W&   & F3 & Freq &   &   & 0.626 & 0.72 & 0.72 & 0.37 \\
Ex8 & Google & 2W&   & F1+F3 & Freq & \checkmark & \checkmark & 0.626 & 0.64 & 0.56 & 0.28 \\
Ex9 & L2+Google & 2W&   & F1+F3 & Freq &   & \checkmark & 0.624 & 0.71 & 0.7 & 0.37 \\
Ex10 & Google & 2W&   & F1 & Freq &   & \checkmark & 0.624 & 0.69 & 0.62 & 0.32 \\
Ex11 & L1+Google & 2W&   & F3 & Freq &   &   & 0.623 & 0.7 & 0.72 & 0.37 \\
Ex12 & L2+Google & 2W&   & F3 & Freq &   & \checkmark & 0.623 & 0.68 & 0.66 & 0.35 \\
Ex13 & L2+Google & 2W&   & F3 & Freq & \checkmark & \checkmark & 0.623 & 0.61 & 0.56 & 0.3 \\
Ex14 & L2+Google & 2W&   & F3 & Freq &   &   & 0.62 & 0.69 & 0.74 & 0.4 \\
Ex15 & L1+Google & 2W& Schema & F1+F3 & Freq &   & \checkmark & 0.617 & 0.69 & 0.66 & 0.34 \\
Ex16 & L2+Google & 2W&   & F1 & Freq &   & \checkmark & 0.616 & 0.68 & 0.62 & 0.32 \\
Ex17 & Google & 2W& Schema & F1+F3 & Freq &   & \checkmark & 0.615 & 0.7 & 0.64 & 0.32 \\
Ex18 & L1 & 2W& Schema & F3 & Freq & \checkmark & \checkmark & 0.614 & 0.65 & 0.6 & 0.32 \\
Ex19 & L1+Google & 2W&   & F1+F3 & Freq &   &   & 0.613 & 0.72 & 0.72 & 0.38 \\
Ex20 & L1+Google & 2W&   & F1+F3 & Freq &   & \checkmark & 0.613 & 0.7 & 0.66 & 0.35 \\
... & ...  & ... & ...  & ...  & ...  & ... & ...  & ...  & ...  & ...  & ...  \\
Ex78 & Google & 2W& Schema & F1+F3 & Gaussian &   & \checkmark & 0.552 & 0.66 & 0.66 & 0.34 \\
Ex80 & L2+Google & 2W& Schema & F1+F3 & Gaussian &   & \checkmark & 0.55 & 0.69 & 0.7 & 0.36 \\
Ex82 & L1 & 2W& Schema & F3 & Gaussian &   & \checkmark & 0.549 & 0.68 & 0.64 & 0.33 \\
... & ...  & ... & ...  & ...  & ...  & ... & ...  & ...  & ...  & ...  & ...  \\
BS2 & Google & 2W&   &   & Freq &   &   & 0.473 & 0.53 & 0.42 & 0.22 \\
... & ...  & ... & ...  & ...  & ...  & ... & ...  & ...  & ...  & ...  & ...  \\
BS1 & Google & 2W&   &   & TFIDF &   &   & 0.063 & 0.08 & 0.06 & 0.03 \\
\end{tabular}
\end{table}

Clear results:

1) the better is this good, which is much better than BS2 and by far better BS1. Tradicional tf - iff measures fail. And more when there are no filter

1) The problem with non strict Named Entities being promoted in TFIDF strategy is clearly alleviated for all filters and combinations. TF strategy has generally improved as well, but in a lower scale.
2) The TF strategy performs better than the current implementation of TFIDF in ALL cases.
3) Best Filter for TF strategy is F3 (0.523665971)
4) Best Filter for TFIDF strategy is the combination F1\_F3
5) I combined the results for both strategies via a) simple multiplication and b) their average value, see rows 4th and 5th. In both cases, the combination F1\_F3 seems to perform the best.m
%%%%%%%%%%%%%%%%%%%%%%%
%%%  5. Conclusion  %%%
%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}
\label{sec:Conclusion}
We presented an approach for context-aware annotating news events, designed to precisely harvest program descriptions starting from named entities recognized in TV video transcripts. Because the entities initially spotted are typically insufficient for covering the broader range of concepts that best describe a particular news clip, we expanded this set by analyzing additional textual documents about the same event.

The evaluation indicates that we can successfully expand the initial set of recognized entities with more relevant concepts not detected by pure named entity recognition approaches and produce a more accurate ranking of important concepts that brings forward entities which user are more interested about.

Future: tailor ranking to particular types of news: sport, politics, regional, international, opinion, etc. Survey on 15 videos from the BBC about international facts. Evaluated over people in different countries.
semantic annotation for doing: summarizing topics/entities


%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Acknowledgments  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Acknowledgments}
This work was partially supported by the European Union's 7th Framework Programme via the project LinkedTV (GA 287911).

%%%%%%%%%%%%%%%%%%%%%%
%%%  Bibliography  %%%
%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{abbrv}
\bibliography{NewsConceptExpansion}

\end{document}
